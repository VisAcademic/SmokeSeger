batch_size: 2
iters: 10
num_workers: 1

train_dataset:
  type: ImgH5Dataset
  dataset_root: ./
  h5_file_path: local_data/smoke_seg.h5
  train_path: local_data/train.list
  num_classes: 2
  transforms:
    - type: Resize
      target_size: [512, 512]
    - type: SegRandAugment
      n: 2
      m: 13
      policy_type: ImageNet
    - type: Normalize
  mode: train

val_dataset:
  type: ImgH5Dataset
  dataset_root: ./
  h5_file_path: local_data/smoke_seg.h5
  val_path: local_data/valid.list
  transforms:
    - type: Resize
      target_size: [ 512, 512 ]
    - type: Normalize
  mode: val
  num_classes: 2

test_dataset:
  type: ImgH5Dataset
  dataset_root: ./
  h5_file_path: local_data/smoke_seg.h5
  val_path: local_data/valid.list
  transforms:
    - type: Resize
      target_size: [ 512, 512 ]
    - type: Normalize
  mode: test
  num_classes: 2

optimizer:
  type: sgd
  momentum: 0.9
  weight_decay: 5.0e-4

lr_scheduler:
  type: LinearWarmup
  learning_rate:
    type: PolynomialDecay
    learning_rate: 0.02
    end_lr: 1.0e-5
    power: 1.0
  warmup_steps: 30
  start_lr: 0.0
  end_lr: 0.001

loss:
  types:
    - type: CrossEntropyLoss
  coef: [1]

model:
  type: SmokeSeger
  num_classes: 2
  decoder:
    type: MlpMixerDecoder
    num_classes: 2
    mlp_channels: 256
    repeat: 1
  #cnn_pretrain: pretrained/hardnet/hardnet_base.pdparams
  #trans_pretrain: pretrained/segformer/segformer_b2_base.pdparams
  #pretrain: pretrained/smokeseger/smokeseger_128508.pdparams

